# Epic 2 Retrospective: Assessment Backend Services

**Date:** 2026-02-06
**Facilitator:** Bob (Scrum Master)
**Epic:** 2 - Assessment Backend Services
**Status:** Complete (13/13 stories done)

---

## Team Participants

- Bob (Scrum Master) — Facilitator
- Alice (PO) — Product perspective
- Winston (Architect) — Architecture review
- Amelia (Dev) — Implementation insights
- Quinn (QA) — Testing and quality assessment
- Vincentlay (Project Lead) — Direction and decisions

---

## Epic Summary

| Metric | Value |
|--------|-------|
| Stories Completed | 13/13 (100%) |
| Total Tests at Epic End | ~301 (API) + ~99 (domain) + ~78 (infrastructure) + ~9 (frontend) + 11 (integration) |
| Production Incidents | 0 |
| Blockers Encountered | 0 hard blockers |
| Tech Debt Items | 5 (see inventory) |
| Code Review Rounds | 6+ (adversarial reviews on multiple stories) |
| Epic Duration | ~7 days (2026-01-30 to 2026-02-05) |

### Stories Delivered

| Story | Title | Agent Model | Tests | Key Output |
|-------|-------|-------------|-------|------------|
| 2.0 | Linter Unification & Frontend Fixes | Not recorded | ~92 | Biome replaces ESLint, `@workspace/lint` shared config |
| 2.0.5 | Effect-Based DI Refactoring | Not recorded | 79 | FiberRef → Context.Tag migration, Layer.effect pattern |
| 2.1 | Session Management & Persistence | Not recorded | N/A | Drizzle ORM, 4 use-cases, 19ms resume (103 msgs) |
| 2.1.1 | GitHub Actions CI/CD & Testing | Claude Haiku 4.5 | 54 | CI pipeline, git hooks, branch protection |
| 2.2 | Nerin Agent Setup & Conversational Quality | Claude Opus 4.5 | 102 | LangGraph StateGraph, PostgresSaver, streaming |
| 2.2-1 | Migrate to Effect Config | Claude Opus 4.5 | 166 | `process.env` → Effect Config, fail-fast startup |
| 2.2.5 | Redis & Cost Management Infrastructure | Claude Opus 4.5 | 136 | ioredis, CostGuard repository, atomic cost tracking |
| 2.3 | Analyzer & Scorer Agent Implementation | Claude Sonnet 4.5 | 301 | Big Five facet analysis, weighted scoring, batch trigger |
| 2.4 | LangGraph State Machine & Orchestration | Claude Opus 4.5 | 126 | Multi-node graph, outlier steering, budget enforcement |
| 2.5 | LLM Cost Tracking & Rate Limiting | Claude Sonnet 4.5 | 111 | Rate limiting (1/day), $75 daily cap, HTTP 429/503 |
| 2.6 | Migrate to @effect/vitest | Claude Sonnet 4.5 | 72 | `it.effect()`, TestClock, pnpm catalog centralization |
| 2.7 | TypeScript Compilation & Code Quality | Claude Haiku 4.5 | 124 | `bundler` resolution, ~143 .js imports stripped, as-any audit |
| 2.8 | Docker Setup for Integration Testing | Claude Sonnet 4.5 | 11 (integration) | compose.test.yaml, MOCK_LLM Layer swapping |

---

## What Went Well

### 1. Hexagonal Architecture Consistency (Winston)

Every story in Epic 2 followed the hexagonal architecture pattern established in Story 2.0.5:

- **Domain** (`packages/domain`): Context.Tag interfaces (ports)
- **Infrastructure** (`packages/infrastructure`): Implementation layers (adapters)
- **Use-Cases** (`apps/api/src/use-cases`): Pure business logic with Effect.gen
- **Handlers** (`apps/api/src/handlers`): Thin HTTP adapters, zero business logic

This was applied consistently to: SessionRepository, MessageRepository, NerinAgentRepository, AnalyzerRepository, ScorerRepository, FacetEvidenceRepository, OrchestratorRepository, CostGuardRepository, RedisRepository, and LoggerRepository. The pattern proved scalable — adding new repositories became mechanical.

### 2. TDD Discipline Across All Stories (Amelia)

Every implementation story followed RED-GREEN-REFACTOR strictly. Tests were written first, failed as expected, then implementations were built to pass them. Test counts grew from ~54 (Story 2.1.1) to ~301 (Story 2.3) organically — no regressions at any point.

Notable TDD wins:
- Story 2.3: 301 tests across 4 packages — Tree of Thoughts analysis selected optimal scoring algorithm before implementation
- Story 2.4: 126 tests including outlier-based steering validation — statistical approach validated by tests before being built
- Story 2.5: Rate limiting edge cases (anonymous users, midnight rollover) caught by upfront test design

### 3. Effect-ts as Unifying Framework (Winston)

Effect-ts proved to be the right foundational choice for a backend of this complexity:

- **Context.Tag DI** (Story 2.0.5): Clean dependency injection without class hierarchies
- **Effect Config** (Story 2.2-1): Fail-fast startup with typed, validated configuration
- **Schema validation** (Story 2.3, 2.4): Single source of truth for LLM response parsing
- **Error handling**: Tagged errors (`BudgetPausedError`, `RateLimitExceeded`, `SessionNotFoundError`) with automatic HTTP status mapping
- **Layer composition**: Production vs test vs mock layers swapped declaratively
- **@effect/vitest** (Story 2.6): `it.effect()` + TestClock for deterministic time-dependent tests

The bootstrap.ts → full Layer DI refactoring (Story 2.2-1) was a particularly impactful improvement — requested by the project lead and resulting in cleaner, fully declarative service composition.

### 4. AI Code Review Effectiveness (Quinn)

Code reviews caught significant issues across multiple stories:

| Story | Issues Found | Critical Fixes |
|-------|-------------|----------------|
| 2.0 | 7 (4 critical, 3 major) | Lint config, import resolution |
| 2.2.5 | 10 (3 critical) | Missing ioredis dep, TTL reset bug, missing ttl() method |
| 2.3 | Code review passed cleanly | N/A |
| 2.4 | 23 (across 2 rounds) | Token tracking, confidence range (0-1→0-100), analyzer schema field, message dedup |
| 2.7 | Multiple | Incorrect `@biome-ignore` format, missing suppression comments |

Story 2.4 (LangGraph Orchestration) had the most intensive review — 13 issues in round 1, 10 in round 2. This was the most architecturally complex story, and the reviews caught real integration bugs that would have surfaced in production.

### 5. Incremental Infrastructure Investment (Bob)

Epic 2 had a smart sequencing of infrastructure stories that paid compounding dividends:

```
2.0 (Biome) → Clean linting foundation
  └→ 2.0.5 (DI) → Context.Tag pattern for everything
       └→ 2.1 (Sessions) → Repository pattern established
            └→ 2.2 (Nerin) → First agent with hexagonal pattern
                 └→ 2.3 (Analyzer/Scorer) → Agents follow established pattern
                      └→ 2.4 (Orchestrator) → Composes all agents
```

By Story 2.4, adding the orchestrator was mostly composition — the hard architectural decisions were already made.

### 6. Agent Model Selection Strategy (Amelia)

The team naturally developed a model selection strategy:

- **Claude Opus 4.5**: Complex architectural stories (2.2, 2.2-1, 2.2.5, 2.4) — high reasoning required
- **Claude Sonnet 4.5**: Implementation-heavy stories (2.3, 2.5, 2.6, 2.8) — good balance of speed and quality
- **Claude Haiku 4.5**: Tooling/configuration stories (2.1.1, 2.7) — fast, sufficient for mechanical tasks

This wasn't planned upfront but emerged organically. The most complex story (2.4: LangGraph Orchestration) correctly used Opus, and it still needed 2 rounds of code review.

### 7. Docker Integration Testing (Quinn)

Story 2.8 established a zero-cost integration testing strategy with `MOCK_LLM=true` Layer swapping:

- 11 integration tests running against a real Docker stack (PostgreSQL + API)
- Mock LLM with 10 pattern-based responses for deterministic testing
- Production-parity environment (same Docker image, same migrations)
- Automatic lifecycle management (`pnpm test:integration` handles Docker up/down)

This validates the full HTTP stack without spending money on LLM API calls.

---

## What Didn't Go Well / Areas for Improvement

### 1. Status Tracking Inconsistencies (Bob)

Three stories have `Status: review` in their story files but are tracked as `done` in sprint-status.yaml:
- Story 2.4 (LangGraph Orchestration)
- Story 2.5 (Cost Tracking & Rate Limiting)
- Story 3.0 (carried from Epic 3)

This indicates the story file status was not consistently updated after code review completion. The sprint-status.yaml is authoritative, but the discrepancy creates confusion.

### 2. Pre-Existing Failing Tests (Quinn)

19 failing tests in `confidence-calculator.service.test.ts` (domain package) have persisted since before Epic 2. They're not in the turbo pipeline so CI passes, but they represent unresolved tech debt. These were identified in the Epic 3 retro as well and remain unfixed.

### 3. Story 2.4 Required Two Code Review Rounds (Quinn)

Story 2.4 (LangGraph Orchestration) required 23 total issues across 2 review rounds. While the reviews were effective, the volume suggests the initial implementation had gaps:

- Round 1 (Task 12): 13 issues — token tracking bugs, schema mismatches, confidence range errors
- Round 2 (Task 13): 10 issues — message deduplication, Effect Schema structured output adoption

The second round introduced a significant architectural change (Effect Schema as single source of truth for agent output validation) that arguably should have been in the original story design.

### 4. No Epic 1 Retrospective Document (Bob)

Epic 1 was marked as having a retrospective completed, but no retrospective document was ever generated. This means learnings from the foundation phase were not formally captured, and there's no baseline to compare Epic 2's improvements against.

### 5. Deferred Items Accumulated (Amelia)

Several items were deferred across stories, creating a trail of incomplete work:

- Story 2.2: P95 latency testing deferred to integration phase
- Story 2.2.5: Railway Redis configuration, health check update, CLAUDE.md deferred to 2.5
- Story 2.5: Story status never updated from `review` to `done`
- Story 2.8: GitHub Actions integration test pipeline deferred to future task

While each deferral was individually reasonable, the accumulation means some stories are technically "done" with known gaps.

### 6. Story Count Scope Creep (Alice)

Epic 2 grew from the original plan to 13 stories. The .5 and .0 stories (2.0, 2.0.5, 2.2-1, 2.2.5) were added mid-epic to address infrastructure needs that weren't fully anticipated:

- 2.0: Linting was a mess from Epic 1
- 2.0.5: DI pattern needed to change before building services
- 2.2-1: `process.env` everywhere needed to become Effect Config
- 2.2.5: Redis infrastructure needed before rate limiting

These were all necessary, but suggest the epic planning phase didn't fully account for infrastructure maturation work.

---

## Key Insights

1. **Hexagonal architecture scales well when patterns are established early.** Stories 2.0.5 and 2.1 set the Context.Tag + Layer.effect pattern that every subsequent story followed. By Story 2.4, adding a new repository was a 10-minute task.

2. **Effect-ts Layer composition is the right abstraction for test infrastructure.** The progression from inline `vi.fn()` mocks → centralized `TestRepositoriesLayer` → per-file `vi.mock()` with `__mocks__` folders (Epic 3) shows the testing strategy maturing alongside the codebase.

3. **Code reviews on complex stories should expect multiple rounds.** Story 2.4 needed 2 rounds with 23 issues. This should be budgeted for — "review" is not a rubber stamp, it's an active quality gate.

4. **Infrastructure stories (.0 and .5 stories) are underrated.** Stories 2.0 (Biome), 2.0.5 (DI), 2.2-1 (Config), and 2.2.5 (Redis) had no user-facing output but enabled every feature story. Future epics should plan 2-3 infrastructure stories upfront.

5. **The LangGraph + Effect bridge pattern works.** Story 2.4 established that LangGraph async operations can be bridged into Effect's type-safe world using `OrchestratorGraphRepository` as the boundary. External code sees only Effect APIs.

6. **Outlier-based steering beats arbitrary thresholds.** Story 2.4's `getSteeringTarget()` uses `confidence < (mean - stddev)` — a statistical approach with zero magic numbers. This is more robust and self-calibrating than hardcoded thresholds.

---

## Previous Retrospective Follow-Through

No Epic 1 retrospective document exists. The `epic-1-retrospective` was marked as `done` in sprint-status.yaml without generating a formal document. No action items to track from Epic 1.

**Recommendation:** Accept this gap and ensure all future retrospectives produce documents.

---

## Technical Debt Inventory

| Item | Source | Severity | Owner | Status |
|------|--------|----------|-------|--------|
| 19 failing `confidence-calculator` tests | Pre-Epic 2 | Medium | Dev | Carry forward |
| Story 2.4/2.5/3.0 status files say `review` not `done` | Epic 2/3 | Low | SM | To fix |
| P95 latency testing never implemented | Story 2.2 | Medium | Dev | Deferred |
| GitHub Actions integration test pipeline | Story 2.8 | Medium | Dev | Deferred |
| Railway Redis configuration undocumented | Story 2.2.5 | Low | Dev | Done in 2.5 |
| `generateOceanCode` branded return type | Epic 3 | Low | Dev | Deferred |
| 4-letter archetype naming (needs 5-letter Phase 2) | Epic 3 | Low | Planned | By design |

---

## Epic 3 Completion Status

Epic 3 (OCEAN Archetype System) has been completed since this retrospective was delayed. See `epic-3-retro-2026-02-06.md` for full details:

- 3 stories, ~496 tests, zero production incidents
- Pure domain functions (`generateOceanCode`, `lookupArchetype`) with exhaustive coverage
- `__mocks__` folder migration improved test infrastructure
- Key agreements: pure function default, TDD mandatory, AI code reviews mandatory

---

## Epic 4 Preparation

### Next Epic: Epic 4 - Frontend Assessment UI (5 stories)

**Stories:**
1. 4.1: Authentication UI (Sign-Up Modal)
2. 4.2: Assessment Conversation Component
3. 4.3: Session Resumption & Device Switching
4. 4.4: Optimistic Updates & Progress Indicator
5. 4.5: Component Documentation with Storybook

### Architectural Decisions

- **ElectricSQL removed** — Per project lead decision. TanStack Query handles server-state caching. No local-first sync needed for MVP.
- **Frontend stack confirmed**: TanStack Start/Router/Query/Form, React 19, shadcn/ui, Tailwind v4, Better Auth client
- **State management**: TanStack Query for server state, React local state for UI state. No Redux/Zustand.
- **Component testing**: Vitest + React Testing Library for unit tests, Storybook for visual documentation (Story 4.5)

### Dependencies on Epic 2

Heavy. Epic 4 consumes the backend APIs built in Epic 2:

- **Better Auth** (Stories 1.2, 2.2-1): Client SDK needed for auth modal (Story 4.1)
- **Assessment endpoints** (Stories 2.1, 2.2, 2.4): Start/send/resume for conversation component (Story 4.2)
- **Session management** (Story 2.1): Resume flow for device switching (Story 4.3)
- **Rate limiting** (Story 2.5): Error handling for 429 responses in UI (Story 4.1)
- **Cost tracking** (Story 2.5): Budget pause handling for 503 responses (Story 4.4)

### Risks & Concerns

1. **Better Auth client SDK + TanStack Start SSR**: The SSR integration pattern needs verification before Story 4.1. Better Auth's React SDK may need adaptation for server-side rendering.
2. **Effect Platform HTTP client**: The frontend needs to call Effect Platform HTTP API contracts. The client-side integration pattern isn't yet established.
3. **Streaming UI**: Story 2.2 established backend streaming but frontend consumption (`graph.stream()` → SSE/WebSocket) isn't designed yet.

### Preparation Checklist

- [ ] Verify Better Auth client SDK setup pattern for TanStack Start SSR
- [ ] Verify API contract definitions in `packages/contracts` cover auth + assessment endpoints
- [ ] Fix 19 failing `confidence-calculator` tests before they surface during domain builds
- [ ] Prototype Effect Platform HTTP client integration pattern
- [ ] Design streaming message delivery for conversation component

---

## Action Items

| # | Action | Owner | Priority | Success Criteria |
|---|--------|-------|----------|-----------------|
| 1 | Fix 19 failing `confidence-calculator` tests | Dev | Medium | All domain package tests pass |
| 2 | Update story file statuses (2.4, 2.5 → done) | SM | Low | Story files match sprint-status.yaml |
| 3 | Verify Better Auth client SDK + TanStack Start SSR | Dev | High | Auth flow prototyped before Story 4.1 |
| 4 | Prototype Effect Platform HTTP client integration | Dev | High | Frontend can call backend contracts type-safely |
| 5 | Add integration tests to GitHub Actions CI | Dev | Medium | CI runs Docker integration tests on PR |
| 6 | Continue AI code reviews on all stories | Team | Ongoing | Code review step in every story |
| 7 | Plan infrastructure stories upfront for Epic 4 | SM/Architect | Medium | .0 stories identified before epic starts |

---

## Team Agreements

- Hexagonal architecture (Context.Tag ports + Layer adapters) is the standard for all backend services
- TDD (RED-GREEN-REFACTOR) remains mandatory for all implementation stories
- AI code reviews continue for all stories — expect 2 rounds for complex stories
- Infrastructure stories (.0 and .5) should be planned upfront, not discovered mid-epic
- Effect-ts Layer composition (not imperative bootstrap) is the standard for service wiring
- ElectricSQL is removed from the frontend stack going forward
- Pure function pattern is the default for new domain utilities (reinforced from Epic 3)
- Story file statuses must be updated to match sprint-status.yaml at story completion

---

## Retrospective Assessment

**Epic 2 Readiness:** Complete. All 13 stories done, ~500+ tests passing across all packages, zero production incidents, hexagonal architecture consistently applied, multi-agent LangGraph pipeline operational.

**Epic 4 Readiness:** Unblocked but with prep work needed. Backend APIs are ready. Two high-priority preparation tasks (Better Auth client SDK verification, Effect RPC client prototype) should be completed before Story 4.1 kicks off.

**Overall:** Epic 2 was the most complex epic in the project — 13 stories spanning linting, DI patterns, session management, CI/CD, three LLM agents, orchestration, cost management, configuration, testing infrastructure, and Docker integration testing. The hexagonal architecture and Effect-ts patterns proved scalable, and the TDD discipline prevented regressions across the entire journey. The main lessons are: plan infrastructure work upfront, budget for multi-round code reviews on complex stories, and keep story file statuses in sync.
